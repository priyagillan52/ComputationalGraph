{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GSoTfLFsau-Q"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06bnDgIYNrim"
      },
      "source": [
        "# Homework 2: Programming\n",
        "\n",
        "The following notebook contains skeleton-code for answering problems 2-4 of homework assignment 2. Please read through each cell carefully to understand what is expected to be implemented. For your final submission, please try to clean up any intermediate outputs used for debugging.\n",
        "\n",
        "\n",
        "For sumbission, you need to submit\n",
        " - this notebook (.ipynb file) with all cell outputs\n",
        " - an exported PDF version with cell outputs of this notebook\n",
        "\n",
        "You need to check that cell outputs are included in your PDF file \\(sometimes the outputs will not be properly shown when exporting\\), and then put them in the same ZIP file and submit to Homework 2-programming on Gradescope.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scoOJkUyNrin"
      },
      "source": [
        "### Imports\n",
        "\n",
        "You should be able to complete the entire assignment using only the following imports. Please consult the course staff if you are unsure about whether additional packages may be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vllQSx4E9oP"
      },
      "source": [
        "## Import Packages\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJX0u5W1cZDc"
      },
      "source": [
        "## Question 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGn3Fryvfrbu"
      },
      "source": [
        "Below we provide an AutoGrad class named `Value`. The basic idea is to store the existing computational map during the creation of each `Value`, and calculate the gradient using backpropagation when one of the `Value` calls `backward()` method.\n",
        "\n",
        "The `backward()` function will arange the computational graph and backpropagate the gradients. All you need to do is to implement all the operations with its corresponding `_backward` function. We have provided the `__add__` function (sum of two nodes) as an example to help get you started.\n",
        "\n",
        "This notebook is designed in a Object Oriented way, if you are not farmiliar with the Object Oriented Programming in Python, you can refer to:\n",
        "\n",
        "(1) https://realpython.com/python3-object-oriented-programming/\n",
        "\n",
        "(2) https://docs.python.org/3/tutorial/classes.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7saLrsq-5Wc6"
      },
      "source": [
        "class Value:\n",
        "\n",
        "    \"\"\"\n",
        "    Basic unit of storing a single scalar value and its gradient\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=()):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        self._prev = set(_children)\n",
        "        self._backward = lambda: None\n",
        "\n",
        "    def __add__(self, other):\n",
        "        \"\"\"\n",
        "        Example implementation of a single class operation (addition)\n",
        "\n",
        "        Args:\n",
        "            other (Any): Node to add with the class\n",
        "\n",
        "        Returns:\n",
        "            out (callable): Function to referesh the gradient\n",
        "        \"\"\"\n",
        "        #Firstly, convert some default value type in python to Value\n",
        "        #Then do operations with two or more Value object\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "\n",
        "        #Secondly, create a new Value object which is the result of the operation\n",
        "        out = Value(self.data + other.data, (self, other))\n",
        "\n",
        "        #Thirdly, create a _backward function for the output object to refresh\n",
        "        # the gradient of its _childrens,\n",
        "        #Then assign this _backward function to the output object.\n",
        "        def _backward():\n",
        "            self.grad += out.grad * 1.0\n",
        "            other.grad += out.grad * 1.0\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        \"\"\"\n",
        "        Multiplication operation (e.g. Value(3) * Value(2) = Value(6))\n",
        "        \"\"\"\n",
        "        #TODO implement multiplication operation\n",
        "\n",
        "        return NotImplementedError\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        \"\"\"\n",
        "        Power operation (e.g Value(3) ** 2 = Value(9))\n",
        "        \"\"\"\n",
        "        #TODO implement power operation, we don't need to convert the exponent to Value\n",
        "        assert isinstance(other, (int, float))\n",
        "\n",
        "        return NotImplementedError\n",
        "\n",
        "    def relu(self):\n",
        "        \"\"\"\n",
        "        ReLU activation function applied to the current Value\n",
        "        \"\"\"\n",
        "        #TODO implement the relu activation function for the value itself.\n",
        "\n",
        "        return NotImplementedError\n",
        "\n",
        "\n",
        "    def exp(self):\n",
        "        \"\"\"\n",
        "        Exponentiate the current Value (e.g. e ^ Value(0) = Value(1))\n",
        "        \"\"\"\n",
        "        #TODO implement the exponential function for and treat the value as exponent.\n",
        "        #The base is natural e, you can use numpy to calculate the value of the exponential.\n",
        "\n",
        "        return NotImplementedError\n",
        "\n",
        "    def log(self):\n",
        "        \"\"\"\n",
        "        Take the natural logarithm (base e) of the current Value\n",
        "        \"\"\"\n",
        "        #TODO implement the logarithm function for and treat the value as exponent.\n",
        "        #The bottom number should be e, you can use numpy to calculate the value of the logarithm.\n",
        "\n",
        "        return NotImplementedError\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Run backpropagation from the current Value\n",
        "        \"\"\"\n",
        "        #This function is called when you start backpropagation from this Value\n",
        "\n",
        "        #The gradient of this value is initialized to 1 for you.\n",
        "        self.grad = 1\n",
        "\n",
        "        #You need to find a right topological order all of the children in the graph.\n",
        "        #As for topology sort, you can refer to http://www.cs.cornell.edu/courses/cs312/2004fa/lectures/lecture15.htm\n",
        "\n",
        "        topo = []\n",
        "        #TODO find the right list of Value to be traversed\n",
        "        '''\n",
        "        Hint: you can recursively visit all non-visited node from the node calling backward.\n",
        "        add one node to the head of the list after all of its children node are visited\n",
        "        '''\n",
        "\n",
        "        #go one variable at a time and apply the chain rule to get its gradient\n",
        "\n",
        "        for v in topo:\n",
        "            v._backward()\n",
        "\n",
        "    # We handled the negation and reverse operations for you\n",
        "    def __neg__(self): # -self\n",
        "        \"\"\"\n",
        "        Negate the current Value\n",
        "        \"\"\"\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): #other + self\n",
        "        \"\"\"\n",
        "        Reverse addition operation (ordering matters in Python)\n",
        "        \"\"\"\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        \"\"\"\n",
        "        Subtraction operation\n",
        "        \"\"\"\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        \"\"\"\n",
        "        Reverse subtraction operation\n",
        "        \"\"\"\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        \"\"\"\n",
        "        Reverse multiplication operation\n",
        "        \"\"\"\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        \"\"\"\n",
        "        Division operation\n",
        "        \"\"\"\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        \"\"\"\n",
        "        Reverse diction operation\n",
        "        \"\"\"\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Class representation (instead of unfriendly memory address)\n",
        "        \"\"\"\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWMte8xaeWpK"
      },
      "source": [
        "Now, we are going to use the simple example in q1.b to get you familar with the usage of this class.\n",
        "\n",
        "If your implementation is correct, you will get the same values and gradients as your hand-caculated ones.\n",
        "\n",
        "Be careful! Even you get this test case right, it does not guarantee the correctness of your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeoL0WIL_sHD"
      },
      "source": [
        "## Initialize Example Values (From Written Assignment)\n",
        "w1 = Value(0.2)\n",
        "w2 = Value(0.4)\n",
        "x1 = Value(-0.4)\n",
        "x2 = Value(0.5)\n",
        "\n",
        "#TODO\n",
        "#Do calculation for the question 1.b, and call backward to start backpropagation.\n",
        "#Then print out the gradient of w1 w2 x1 x2.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oowriEHef1b7"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcv2k0PLg6wY"
      },
      "source": [
        "### Implementation of the linear layer\n",
        "You will implement a `LinearLayer` module here.\n",
        "\n",
        "We provide the initialization of the class `LinearLayer`. You need to implement the forward function -- Return the results - `out` with the shape `[n_samples, n_out_channels]` of a linear layer when the the data `x` shaped `[n_samples, n_in_channels]` is fed into it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTHmcr7pLbU0"
      },
      "source": [
        "class Module:\n",
        "\n",
        "    \"\"\"\n",
        "    Base Model Module\n",
        "    \"\"\"\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "class LinearLayer(Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Linear Layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nin, nout):\n",
        "        \"\"\"\n",
        "        Here we randomly initilize the weights w as 2-dimensional list of Values\n",
        "        And b as 1-dimensional list of Values with value 0\n",
        "\n",
        "        You may use this stucture to implement the __call__ function\n",
        "        \"\"\"\n",
        "        self.w = []\n",
        "        for i in range(nin):\n",
        "            w_tmp = [Value(random.uniform(-1,1)) for j in range(nout)]\n",
        "            self.w.append(w_tmp)\n",
        "        self.b = [Value(0) for i in range(nout)]\n",
        "        self.nin = nin\n",
        "        self.nout = nout\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (2d-list): Two dimensional list of Values with shape [batch_size , nin]\n",
        "\n",
        "        Returns:\n",
        "            xout (2d-list): Two dimensional list of Values with shape [batch_size, nout]\n",
        "        \"\"\"\n",
        "        #TODO implement this function and return the output of a linear layer.\n",
        "        return NotImplementedError\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Get the list of parameters in the Linear Layer\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            params (list): List of parameters in the layer\n",
        "        \"\"\"\n",
        "        return [p for row in self.w for p in row] + [p for p in self.b]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gwYpbiRfoFH"
      },
      "source": [
        "Test your implementation of linear layer, the error should be nearly 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHLnHUW-fydy"
      },
      "source": [
        "## Initialization of Layer with Weights\n",
        "linear_model_test = LinearLayer(4, 4)\n",
        "linear_model_test.w = [[Value(data=0.7433570245252463), Value(data=-0.9662164096144394), Value(data=-0.17087204941322653), Value(data=-0.5186656374983067)],\n",
        "                       [Value(data=-0.1414882837892344), Value(data=-0.5898971049017006), Value(data=-0.3448340220492381), Value(data=0.5278833226346107)],\n",
        "                       [Value(data=0.3990701306597799), Value(data=-0.3319058654296163), Value(data=-0.784797384411202), Value(data=0.7603317495966846)],\n",
        "                       [Value(data=-0.5711035064293541), Value(data=-0.0001937643033362857), Value(data=0.12693226232877053), Value(data=-0.36044237239197097)]]\n",
        "linear_model_test.b = [Value(data=0), Value(data=0), Value(data=0), Value(data=0)]\n",
        "\n",
        "## Forward Pass\n",
        "x_test = [[-0.17120438454836173, -0.3736077734087335, -0.48495413054653214, 0.8269206715993096]]\n",
        "y_hat_test = linear_model_test(x_test)\n",
        "y_ref = [[Value(data=-0.7401928625441141), Value(data=0.5466095223360173), Value(data=0.6436403600545564), Value(data=-0.7752067527386406)]]\n",
        "\n",
        "## Error Calculation\n",
        "predict_error = 0\n",
        "for i in range(4):\n",
        "    predict_error += (y_hat_test[0][i] - y_ref[0][i])**2\n",
        "print(predict_error.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfSXiqRShoM1"
      },
      "source": [
        "### Implementation of Loss functions\n",
        "\n",
        "You will implement softmax, cross entropy loss, and accuracy here for further use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW_OLYJ_hhbY"
      },
      "source": [
        "def softmax(y_hat):\n",
        "    \"\"\"\n",
        "    Softmax computation\n",
        "\n",
        "    Args:\n",
        "        y_hat (2d-list): 2-dimensional list of Values with shape [batch_size, n_class]\n",
        "\n",
        "    Returns:\n",
        "        s (2d-list): 2-dimensional list of Values with the same shape as y_hat\n",
        "    \"\"\"\n",
        "    #TODO implement the softmax function and return the output.\n",
        "\n",
        "    return NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xdTC9Tym6sP"
      },
      "source": [
        "def cross_entropy_loss(y_hat, y):\n",
        "    \"\"\"\n",
        "    Cross-entropy Loss computation\n",
        "\n",
        "    Args:\n",
        "        y_hat (2d-list): Output from linear function with shape [batch_size, n_class]\n",
        "        y (1d-list): List of ground truth labels with shape [batch_size, ]\n",
        "\n",
        "    Returns:\n",
        "        loss (Value): Loss value of type Value\n",
        "    \"\"\"\n",
        "    #TODO implement the calculation of cross_entropy_loss between y_hat and y.\n",
        "    return NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jeK31K85qsB"
      },
      "source": [
        "def accuracy(y_hat, y):\n",
        "    \"\"\"\n",
        "    Accuracy computation\n",
        "\n",
        "    Args:\n",
        "        y_hat (2d-list): Output from linear function with shape [batch_size, n_class]\n",
        "        y (1d-list): List of ground truth labels with shape [batch_size, ]\n",
        "\n",
        "    Returns:\n",
        "        acc (float): Accuracy score\n",
        "    \"\"\"\n",
        "    #TODO implement the calculation of accuracy of the predicted y_hat w.r.t y.\n",
        "    return NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jti1kRlzf14E"
      },
      "source": [
        "Test the implementation of `softmax()` and `cross_entropy_loss()` as well as the gradient calculation of `Value` class. The errors should be nearly 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My_zbUj7gBLo"
      },
      "source": [
        "## Ground Truth + Forward Pass\n",
        "y_gt = [1]\n",
        "y_hat_test = linear_model_test(x_test)\n",
        "\n",
        "## Softmax Calculation\n",
        "prob_test = softmax(y_hat_test)\n",
        "prob_ref = [[0.10441739448437284, 0.37811510516540814, 0.4166428991676558, 0.10082460118256342]]\n",
        "softmax_error = 0\n",
        "for i in range(4):\n",
        "    softmax_error += (prob_ref[0][i] - prob_test[0][i])**2\n",
        "print(softmax_error.data)\n",
        "\n",
        "## Cross Entropy Loss Calculation\n",
        "loss_test = cross_entropy_loss(y_hat_test, y_gt)\n",
        "loss_ref = Value(data=0.9725566186970217)\n",
        "print((loss_test - loss_ref).data)\n",
        "\n",
        "## Update Gradient Based on Loss\n",
        "linear_model_test.zero_grad()\n",
        "loss_test.backward()\n",
        "w_gradient_ref = [[-0.017876715758840547, 0.10646942068007896, -0.07133109112844363, -0.01726161379279479],\n",
        "                  [-0.0390111502584479, 0.23234103087567629, -0.1556610258645873, -0.03766885475264107],\n",
        "                  [-0.05063764675610328, 0.30158564847453107, -0.2020526949142369, -0.04889530680419089],\n",
        "                  [0.08634490197366762, -0.5142494748940867, 0.3445306259968013, 0.08337394692361787]]\n",
        "b_gradient_ref = [0.10441739448437282, -0.6218848948345919, 0.4166428991676557, 0.1008246011825634]\n",
        "\n",
        "## Compute Error\n",
        "w_gradient_error = 0\n",
        "b_gradient_error = 0\n",
        "for i in range(4):\n",
        "    b_gradient_error += (linear_model_test.b[i].grad - b_gradient_ref[i]) ** 2\n",
        "    for j in range(4):\n",
        "        w_gradient_error += (linear_model_test.w[i][j].grad - w_gradient_ref[i][j]) ** 2\n",
        "print(w_gradient_error)\n",
        "print(b_gradient_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovsoGs3_ljNC"
      },
      "source": [
        "Implement the following functions to visualize the ground truth and the decision boundary in the same figure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9kPT-caWUaw"
      },
      "source": [
        "def plot_points(X, Y, scale, n, data):\n",
        "    \"\"\"\n",
        "    Plot points in the visualization image\n",
        "    \"\"\"\n",
        "    points_color = [[0., 0. , 255.], [255., 0., 0.], [0., 255., 0.],[0., 0. , 0.]]\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        #TODO Assign a color to \"data\" according to the position and the label of X\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def plot_background(scale, n, model):\n",
        "    \"\"\"\n",
        "    Color the background in the visualization image\n",
        "    \"\"\"\n",
        "\n",
        "    background_color = [[0., 191., 255.], [255., 110., 180.], [202., 255., 112.],[156., 156., 156.]]\n",
        "\n",
        "    data = np.zeros((n,n,3), dtype='uint8')\n",
        "\n",
        "    for i in range(n):\n",
        "        x1 = -scale + 2 * scale / n * i\n",
        "        for j in range(n):\n",
        "            x2 = -scale + 2 * scale / n * j\n",
        "            input = [[Value(x1),Value(x2)]]\n",
        "            #TODO using the model to predict a class for the input and assign a color to \"data\" at this position.\n",
        "    return data\n",
        "\n",
        "\n",
        "def visualization(X, Y, model):\n",
        "    \"\"\"\n",
        "    Decision boundary visualization\n",
        "    \"\"\"\n",
        "    scale = 4.5  # the scale of X axis and Y axis. To say, x is from -scale to +scale\n",
        "    n = 300      # seperate the image into n*n pixels\n",
        "\n",
        "    data = plot_background (scale, n, model)\n",
        "    data = plot_points (X, Y, scale, n, data)\n",
        "\n",
        "    plt.imshow(data)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRJgvFM6Ob-V"
      },
      "source": [
        "if you implement the plot function correctly, you will get some image like:\n",
        "\n",
        "![download.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaPMBVcZiVah"
      },
      "source": [
        "### Implementation of training procedure\n",
        "\n",
        "With input data `x`, ground_truth `y`, and `model` as parameters, implement the gradient descent method to train your model and plot loss and accuracy vs training iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9b1UnDTO12e"
      },
      "source": [
        "def train(x,\n",
        "          y,\n",
        "          model,\n",
        "          loss_function=cross_entropy_loss,\n",
        "          accuracy_function=accuracy,\n",
        "          max_iteration=500,\n",
        "          learning_rate=1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "       x (2-d list): List of Values with shape: [n_samples, n_channels]\n",
        "       y (1-d list): List of integers with shape: [n_samples]\n",
        "       model (Module): Linear model\n",
        "       loss_function (callable): Loss function to use during training\n",
        "       accuracy_function (callable): Function used for calculating training accuracy\n",
        "       max_iteration (int): Number of epochs to train model for\n",
        "       learning_rate (numeric): Step size of the gradient update\n",
        "    \"\"\"\n",
        "    for i in range(max_iteration):\n",
        "        #TODO compute y_hat and calculate the loss between y_hat and y as well as\n",
        "        # the accuracy of y_hat w.r.t y.\n",
        "        y_hat =\n",
        "        loss =\n",
        "        acc =\n",
        "\n",
        "        #TODO Then You will need to calculate gradient for all parameters, and\n",
        "        #do gradient descent for all the parameters.\n",
        "        #The list of parameters can be easily obtained by calling\n",
        "        #model.parameters() which is implemented above.\n",
        "\n",
        "\n",
        "        #Then plot the loss / accuracy vs iterations.\n",
        "        if i % 20 == 19:\n",
        "            print(\"iteration\",i,\"loss:\",loss.data, \"accuracy:\",acc)\n",
        "        ## record loss\n",
        "        if i == 0 :\n",
        "        # initialize L\n",
        "            L = loss.data\n",
        "            A = acc\n",
        "        else:\n",
        "            L = np.append(L,loss.data)\n",
        "            A = np.append(A,acc)\n",
        "\n",
        "    ## Plot Loss and Accuracy\n",
        "    fig0=plt.figure(0)\n",
        "    plt.plot(L,'-')\n",
        "    plt.xlabel('Iteration', fontsize=18)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.show()\n",
        "    fig1=plt.figure(1)\n",
        "    plt.plot(A,'-')\n",
        "    plt.xlabel('Iteration', fontsize=18)\n",
        "    plt.ylabel('Accuracy', fontsize=16)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9p8QtQJjeAl"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Load the data, format it, instantiate your model and start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOHM_FgtoUyu"
      },
      "source": [
        "## Load Q3 Dataset\n",
        "datapath = './Q3_data.npz'\n",
        "data = np.load(datapath)\n",
        "\n",
        "## Load Data and Parse Shape Information\n",
        "X = data['X']\n",
        "Y = data['Y']\n",
        "print(X.shape, Y.shape, np.unique(Y))\n",
        "nin = X.shape[1]\n",
        "nout = np.max(Y) + 1\n",
        "\n",
        "## Initialize data using your Value class\n",
        "x = [[Value(v) for v in sample] for sample in X]\n",
        "y = [int(v) for v in Y]\n",
        "\n",
        "## Initialize a Linear Model\n",
        "linear_model = LinearLayer(nin, nout)\n",
        "\n",
        "## Train the Model using Your Data\n",
        "train(x, y, linear_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEUUcgHEM6of"
      },
      "source": [
        "## Visualize learned decision boundaries\n",
        "visualization(X, Y, linear_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctXkntnIkdjt"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFoPi3NskoIh"
      },
      "source": [
        "### a) Is this dataset linear separable?\n",
        "load the dataset for this question and train a linear model on this dataset and report the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1BHFxm4Ix2c"
      },
      "source": [
        "## Load Q4 Dataset\n",
        "datapath = './Q4_data.npz'\n",
        "data = np.load(datapath)\n",
        "\n",
        "## Parse Data and Identify Dimensions\n",
        "X = data['X']\n",
        "Y = data['Y']\n",
        "nin = X.shape[1]\n",
        "nout = int(np.max(Y)) + 1\n",
        "\n",
        "## Initialize data using your value class\n",
        "x = [[Value(v) for v in sample] for sample in X]\n",
        "y = [int(v) for v in Y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlFvA3CCS33X"
      },
      "source": [
        "## Initialize Linear Model\n",
        "linear_model = LinearLayer(nin, nout)\n",
        "\n",
        "## Train Model\n",
        "train(x, y, linear_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cFD2eQ2WWUQ"
      },
      "source": [
        "## Visualize Learned Decision Boundary\n",
        "visualization(X, Y, linear_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYoFsdm0m18J"
      },
      "source": [
        "### b) Implementation of Multi Layer Perceptron (MLP)\n",
        "\n",
        "Implement a class `MLP` to add arbitrary layers. You will need to implement the forward function to return results `out` with `x` fed into the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldxalUrYLOZW"
      },
      "source": [
        "class MLP(Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Multi Layer Perceptron\n",
        "    \"\"\"\n",
        "    def __init__(self, dimensions):\n",
        "        \"\"\"\n",
        "        Initialize multiple layers here in the list named self.linear_layers\n",
        "        \"\"\"\n",
        "        assert isinstance(dimensions, list)\n",
        "        assert len(dimensions) > 2\n",
        "        self.linear_layers = []\n",
        "        for i in range(len(dimensions) - 1):\n",
        "            self.linear_layers.append(LinearLayer(dimensions[i], dimensions[i+1]))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (2d-list): Two dimensional list of Values with shape [batch_size , nin]\n",
        "\n",
        "        Returns:\n",
        "            xout (2d-list): Two dimensional list of Values with shape [batch_size, nout]\n",
        "        \"\"\"\n",
        "        #TODO Implement this function and return the output of a MLP\n",
        "        return NotImplementedError\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"\n",
        "        Get the parameters of each layer\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            params (list of Values): Parameters of the MLP\n",
        "        \"\"\"\n",
        "        return [p for layer in self.linear_layers for p in layer.parameters()]\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Zero out the gradient of each parameter\n",
        "        \"\"\"\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrFd0boknj94"
      },
      "source": [
        "Train your MLP model and visualize the decision boundary with ground truth points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_zTwbDZQ4I4"
      },
      "source": [
        "## Initialize MLP with Given Parameters\n",
        "mlp_model = MLP([nin, 40, nout])\n",
        "\n",
        "## Train the MLP\n",
        "train(x, y, mlp_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr2-BMquWPg_"
      },
      "source": [
        "## Visualize Decision Boundaries\n",
        "visualization(X, Y, mlp_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSoTfLFsau-Q"
      },
      "source": [
        "## Acknowledgement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9tFA4DZa4E6"
      },
      "source": [
        "The design of the auto grade structure are based on the work https://github.com/karpathy/micrograd"
      ]
    }
  ]
}